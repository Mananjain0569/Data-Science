{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "**Fuction-using-feature-extraction-1\n",
        "January 23, 2024      \n",
        "0.1 ##Reducing Features Using Principal Components     \n",
        "To Learn PCA :          \n",
        "https://towardsdatascience.com/principal-component-analysis-for-dimensionality-reduction-        \n",
        "115a3d157bad        \n",
        "https://machinelearningmastery.com/principal-components-analysis-for-dimensionality-reduction-       \n",
        "in-python/       \n",
        "https://www.kdnuggets.com/2020/05/dimensionality-reduction-principal-component-\n",
        "analysis.html  \n",
        "Problem      \n",
        "Given a set of features, you want to reduce the number of features while retaining the variance in the data.       \n",
        "Solution       \n",
        "Use principal component analysis with scikit's PCA:\n",
        "**"
      ],
      "metadata": {
        "id": "lQOG42GE__Oo"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hn_5vQWH_hKy",
        "outputId": "205180b6-2c2d-4712-a1ac-1ef86f334d98"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Original number of features: 64\n",
            "Reduced number of features: 54\n"
          ]
        }
      ],
      "source": [
        "# Load libraries\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.decomposition import PCA\n",
        "from sklearn import datasets\n",
        "\n",
        "# Load the data\n",
        "digits = datasets. load_digits()\n",
        "\n",
        "# Standardize the feature matria\n",
        "X = StandardScaler().fit_transform(digits.data)\n",
        "\n",
        "# Create a PCA that will retain 99% of the variance\n",
        "pca = PCA(n_components=0.99, whiten=True)\n",
        "\n",
        "# Conduct PCA\n",
        "X_pca = pca.fit_transform(X)\n",
        "\n",
        "# Show results\n",
        "print('Original number of features:', X.shape[1])\n",
        "print('Reduced number of features:', X_pca.shape[1])"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**PCA  Documentation\n",
        "##Reducing Features When Data Is Linearly Inseparable\n",
        "Problem       \n",
        "You suspect you have linearly inseparable data and want to reduce the dimensions.       \n",
        "Solution        \n",
        "Use an extension of principal component analysis that uses kernels to allow for non-linear dimen- sionality reduction**"
      ],
      "metadata": {
        "id": "h1e3ZzSQAlHm"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Load libraries\n",
        "from sklearn.decomposition import PCA, KernelPCA\n",
        "from sklearn.datasets import make_circles\n",
        "\n",
        "# Create linearly inseparable data\n",
        "X, _= make_circles(n_samples=1000, random_state=1, noise=0.1, factor=0.1)\n",
        "\n",
        "#Apply kernal PCA with radius basis function (RBF) kernel\n",
        "kpca = KernelPCA(kernel=\"rbf\", gamma=15, n_components=1)\n",
        "X_kpca = kpca.fit_transform(X)\n",
        "\n",
        "print('Original number of fea')"
      ],
      "metadata": {
        "id": "1907OPRVAutG"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}